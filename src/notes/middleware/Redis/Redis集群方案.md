---
title: "Redis集群方案"
date: 2024-04-14
---

## 1、主从复制：

### 简述：

主从复制其实就是隔离开读和写的，其有一个主节点和多个从节点。主节点负责写，从节点负责读

### 特点：

- 只有一个主节点，多个从节点
- 从节点和级联(Slave 可挂载其他 Slave)。
- 主节点负责写，从节点负责读+数据同步。
- 数据单向同步(主->从)，默认异步复制。

### 诞生原因：

- 单体Redis服务读写是在一起的，但是实际需求中，读和写的操作频次是不一致的，往往读的请求会更多。
- 单体Redis服务运行期间要是出现了故障，其服务就完全不可用了，对于线上业务而言，这个是完全无法接受的。
- 单体Redis服务数据只有一份，就算Redis服务做了持久化配置，但是如果其对应服务器的磁盘出现问题导致数据丢失，这些数据就无法挽回了。严重的可能导致对应生产业务断档。

### 启动流程：

1. 服务使用redis.conf进行配置主从节点。
2. 从节点注册到主节点上【配置主节点IP、端口、密码】
3. 运行服务【主+从(从节点可动态新增)】
4. 从节点向主节点发送`SYNC`命令【Redis2.8+使用`PSYNC`优化增量同步】，请求进行数据同步
5. 主节点执行`BGSAVE`生成RDB文件，发送给从节点【并记录生成期间的新增命令】
6. 主节点将命令写入复制积压缓冲区`repl_backlog`，从节点持续接收增量数据
7. 主节点将所有新写入命令实时发送给从节点

### 配置规则：

- 从节点指定其所依赖的主节点或级联的从节点。
- 主节点设置`min-slaves-to-write`和`min-slaves-max-lag`来确保写入的安全性。

### 请求方式：

- 客户端直接读写主节点，或从节点（需业务实现读写分离）

### 故障转移：

- 无选举方式，主节点故障需要手动恢复（或依赖哨兵模式）

### 优点：

- 读写分离，且同时也做到了数据备份。
- 支持读能力的拓展。

### 缺点：

- 主节点单点故障需人工干预
- 异步复制可能导致数据不一致
- 主节点单点，写性能受限

### 注意点：

- 从节点过多会增加主节点同步压力
- 避免主节点同时作为从节点（级联需谨慎）



## 2、哨兵模式（Sentinel）：

### 简述：

Sentinel模式下的Redis集群拥有Sentinel节点进行服务的监控，其可以进行自动容灾，如果主节点故障了，其会在其从节点中根据对应的有限原则选出主节点并恢复集群的写能力。

### 特点：

- 监控主节点健康状态，实现自动故障转移。
- 哨兵节点独立部署，自身无数据存储。

### 诞生原因：

- 解决主从复制模式下，主节点故障转移问题。
- 提供高可用性保障。

### 启动流程：

1. 启动主节点和从节点
2. 启动多个 Sentinel 进程，配置监控主节点（`sentinel monitor <master-name> <ip> <port> <quorum>`）
3. Sentinel 集群通过 Gossip 协议通信，判定主节点故障后触发选举和切换。

### 配置规则：

- 至少三个 Sentinel 节点（推荐奇数）
- `quorum`参数定义故障判定最小投票数。

### 请求方式：

- 客户端连接`Sentinel`获取主节点地址（需支持Sentine1协议的客户端库）

### 选举方式：

- Sentinel 节点间基于 Raft 协议选举 Leader ，由 Leader 执行故障转移。

### 优点：

- 自动故障转移，高可用性
- 兼容主从模式的读写分离

### 缺点：

- 不支持数据分片，容量存储仍受单机限制
- 配置复杂度高

### 注意点：

- Sentinel 节点应为奇数（如3/5个）避免脑裂。
- 网络分区可能导致误切，需合理设置超时参数。

## 3、集群模式（Cluster）：

### 简述：

去中心化的Redis集群方式，有自动容灾能力。

### 特点：

- 数据分片（16384 个哈希槽），支持水平拓展。
- 节点间 Gossip 协议通信，自动故障转移。

### 诞生原因：

- 解决单机内存和性能瓶颈。
- 实现分布式存储和高并发读写。

### 启动流程：

1. 启动多个Redis示例并启用集群模式（`cluster-enabled yes`）。
2. 使用`cluster meet`命令将节点加入到集群。
3. 分配哈希槽（`cluster addlots`）或自动分配。

### 配置规则：

- 每个节点需配置`cluster-node-timeout`（故障判定时间）。
- 至少三个主节点（推荐6节点：3主+3从）。

### 请求方式：

- 客户端直连任意节点，通过重定向（MOVED/ASK）访问正确分片。
- Smart Client 可缓存槽位映射，减少重定向。

### 选举方式：

- 主节点故障时，从节点发起选举（基于 Raft 变种算法）成为新主。

### 优点：

- 支持水平拓展和高并发。
- 自动数据分片和故障转移。

### 缺点：

- 客户端需兼容集群协议。
- 迁移槽位可能阻塞请求。
- 实务和跨槽位操作受限（需使用 Hash Tag）。

### 注意点：

- 避免大规模数据迁移导致性能抖动。
- 主从节点应分散在不同物理机，避免同时故障。
- 集群规模建议不超过 1000 节点。



## **总结对比**

| 模式     | 适用场景           | 数据一致性 | 扩展性     | 复杂度 |
| :------- | :----------------- | :--------- | :--------- | :----- |
| 主从复制 | 读写分离、数据备份 | 最终一致   | 垂直扩展读 | 低     |
| 哨兵模式 | 高可用主从架构     | 最终一致   | 无分片     | 中     |
| 集群模式 | 大数据量、高并发   | 分片内一致 | 水平扩展   | 高     |
